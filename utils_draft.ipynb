{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalexu97/Projects/Diabetic-Retinopathy-Classification/dr_clf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import f1_score #, kappa\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load accuracy evaluator\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, file_name,\n",
    "                          classes=[0, 1, 2, 3, 4],\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the title if not provided\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "\n",
    "    # Normalize confusion matrix if required\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "                              \n",
    "    # Set ticks and labels\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(f'../results/conf_matrix_{file_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Define function to calculate per class accuracy\n",
    "def calculate_per_class_accuracy(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculate per-class accuracy from a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    - confusion_matrix (numpy.ndarray): The confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - per_class_accuracy (list): List of per-class accuracy values.\n",
    "    \"\"\"\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    per_class_accuracy = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP = confusion_matrix[i, i]\n",
    "        FN = np.sum(confusion_matrix[i, :]) - TP\n",
    "        FP = np.sum(confusion_matrix[:, i]) - TP\n",
    "        TN = np.sum(confusion_matrix) - (TP + FP + FN)\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        per_class_accuracy.append(accuracy)\n",
    "\n",
    "    return per_class_accuracy\n",
    "\n",
    "# Define function to compute metrics\n",
    "def get_compute_metrics(save_cm=True, cm_path=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the function to compute evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "    - pretrained_model_name (str): Name of the pretrained model.\n",
    "    - dataset_name (str): Name of the dataset.\n",
    "    - save_cm (bool): Whether to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "    - compute_metrics (function): Function to compute evaluation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions_proba, labels = eval_pred\n",
    "        predictions = np.argmax(predictions_proba, axis=1)\n",
    "        predictions = np.clip(predictions, 0, 4)\n",
    "        result_accuracy = accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "        if predictions_proba.shape[1] > 1:  # Check if we have more than one class\n",
    "            predictions_proba = torch.nn.functional.softmax(torch.tensor(predictions_proba), dim=-1).numpy()\n",
    "\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        perclass_acc = calculate_per_class_accuracy(cm)\n",
    "\n",
    "        result = {\n",
    "                'accuracy': np.mean([result_accuracy['accuracy']]),\n",
    "                'kappa': np.mean([cohen_kappa_score(labels, predictions, weights = \"quadratic\")]),\n",
    "                'f1': np.mean([f1_score(labels, predictions, average='weighted')]),\n",
    "                # 'roc_auc': np.mean([roc_auc_score(labels, predictions_proba, multi_class='ovr')]),\n",
    "                'roc_auc': np.mean([roc_auc_score(labels, predictions_proba, multi_class='ovr')]),\n",
    "                'class_0' : perclass_acc[0],\n",
    "                'class_1' : perclass_acc[1],\n",
    "                'class_2' : perclass_acc[2],\n",
    "                'class_3' : perclass_acc[3],\n",
    "                'class_4' : perclass_acc[4],\n",
    "                }\n",
    "        \n",
    "        if save_cm:\n",
    "            plot_confusion_matrix(labels, predictions, normalize=True,\n",
    "                            title='Normalized confusion matrix', file_name=cm_path)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "# Define function to define collate function\n",
    "def get_collate_fn(with_embedings=False):\n",
    "    def collate_fn(batch):\n",
    "        collated_batch = {\n",
    "            'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "            'labels': torch.tensor([x['label'] for x in batch])\n",
    "        }\n",
    "        if with_embedings:\n",
    "            collated_batch['embedings'] = torch.stack([x['embedings'] for x in batch])\n",
    "        \n",
    "        return collated_batch\n",
    "    \n",
    "    return collate_fn\n",
    "\n",
    "def build_trainer(model, train_dataset, valid_dataset, args, train_mode=True):\n",
    "    # arguments for training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir, #\"./SSiT-base\",\n",
    "        evaluation_strategy=args.evaluation_strategy, #\"steps\",\n",
    "        logging_steps=args.logging_steps, #50,\n",
    "\n",
    "        save_steps=args.save_steps, #50,\n",
    "        eval_steps=args.eval_steps, #50,\n",
    "        save_total_limit=args.save_total_limit, #3,\n",
    "        \n",
    "        report_to=None, #args.report_to, #\"wandb\",  # enable logging to W&B\n",
    "        run_name=args.run_name, #r_name,  # name of the W&B run (optional)\n",
    "        \n",
    "        remove_unused_columns=False,\n",
    "        dataloader_num_workers = args.dataloader_num_workers, #16,\n",
    "        lr_scheduler_type = args.lr_scheduler_type, #'constant_with_warmup', # 'constant', 'cosine'\n",
    "        \n",
    "        learning_rate=args.learning_rate, #2e-5,\n",
    "        # label_smoothing_factor = 0.6,\n",
    "        per_device_train_batch_size=args.batch_size, #64,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps, #4,\n",
    "        per_device_eval_batch_size=args.batch_size, #64,\n",
    "        num_train_epochs=args.num_train_epochs, #15,\n",
    "        warmup_ratio=args.warmup_ratio, #0.02,\n",
    "        \n",
    "        metric_for_best_model=args.metric_for_best_model, #\"kappa\", # select the best model via metric kappa\n",
    "        greater_is_better = True,\n",
    "        load_best_model_at_end=True,\n",
    "        \n",
    "        push_to_hub=False\n",
    "    )\n",
    "\n",
    "    collate_fn = get_collate_fn(with_embedings=False)\n",
    "    if train_mode:\n",
    "        compute_metrics_f = get_compute_metrics()\n",
    "    else:\n",
    "        cm_path = f'{args.plots_path}/{args.run_name}.png'\n",
    "        compute_metrics_f = get_compute_metrics(save_cm=True, cm_path=cm_path)\n",
    "\n",
    "    # define trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics_f,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def train(model, train_dataset, valid_dataset, args):\n",
    "    \n",
    "    trainer = build_trainer(model, train_dataset, valid_dataset, args, train_mode=True)\n",
    "    train_results = trainer.train()\n",
    "\n",
    "    if args.save_hgf_model:\n",
    "        trainer.save_model()\n",
    "        trainer.log_metrics(\"train\", train_results.metrics)\n",
    "        trainer.save_metrics(\"train\", train_results.metrics)\n",
    "        trainer.save_state()\n",
    "        model.save_pretrained(f\"{args.saved_model_dir}/{args.run_name}\", from_pt=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def test(model, train_dataset, valid_dataset, test_dataset, args):\n",
    "    trainer = build_trainer(model, train_dataset, valid_dataset, args, train_mode=False)\n",
    "    metrics = trainer.evaluate(test_dataset)\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main for train\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('model')\n",
    "\n",
    "from model.classifier import ClfConfig, Classifier\n",
    "# from utils import train, test\n",
    "from data.data_utils import build_datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "def train_main():\n",
    "\n",
    "    # import os\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # is need to train on 'hachiko'\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output_dir', default=\"./Classifier\")\n",
    "    parser.add_argument('--evaluation_strategy', default=\"steps\")\n",
    "    parser.add_argument('--logging_steps', default=50)\n",
    "    parser.add_argument('--save_steps', default=50)\n",
    "    parser.add_argument('--eval_steps', default=50)\n",
    "    parser.add_argument('--save_total_limit', default=3)\n",
    "    parser.add_argument('--report_to', default= None) #\"wandb\")\n",
    "    parser.add_argument('--run_name', default=\"clf_test\")\n",
    "    parser.add_argument('--dataloader_num_workers', default=16)\n",
    "    parser.add_argument('--lr_scheduler_type', default=\"linear\")\n",
    "    parser.add_argument('--learning_rate', default=2e-5)\n",
    "    parser.add_argument('--batch_size', default=16)\n",
    "    parser.add_argument('--gradient_accumulation_steps', default=4)\n",
    "    parser.add_argument('--num_train_epochs', default=15)\n",
    "    parser.add_argument('--warmup_ratio', default=0.02)\n",
    "    parser.add_argument('--metric_for_best_model', default=\"kappa\")\n",
    "    parser.add_argument('--dataset_root_dir', default=\"data/local_datasets\")\n",
    "    # parser.add_argument('--with_emdedings', default=None)\n",
    "    parser.add_argument('--emb_model_checkpoint', default=\"checkpoints/pretrained_vits_imagenet_initialized.pt\")\n",
    "    parser.add_argument('--plots_path', default=\"src\")\n",
    "    parser.add_argument('--save_hgf_model', default=True)\n",
    "    parser.add_argument('--saved_model_dir', default=\"checkpoints\")\n",
    "    parser.add_argument('--backbone_name', default=\"resnet50\")\n",
    "    parser.add_argument('--num_classes', default=5)\n",
    "    parser.add_argument('--input_size', default=224)\n",
    "    parser.add_argument('--pretrained', default=True)\n",
    "    parser.add_argument('--external_embedings', default=False)\n",
    "    parser.add_argument('--external_embedings_len', default=384)\n",
    "    parser.add_argument('--feat_concat', default=True)\n",
    "    parser.add_argument('--load_backbone', default=False)\n",
    "    parser.add_argument('--backbone_checkpoint_path_load', default=\"checkpoints/resnet50_backbone.pt\")\n",
    "    parser.add_argument('--dataset_name', default=\"DDR\")\n",
    "    parser.add_argument('--backbone_checkpoint_path_save', default=\"checkpoints/resnet50_backbone.pt\")\n",
    "    parser.add_argument('--test_after_train', default=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_config = ClfConfig(backbone_name = args.backbone_name,\n",
    "        num_classes = args.num_classes,\n",
    "        input_size = args.input_size,\n",
    "        pretrained = args.pretrained,\n",
    "        external_embedings = args.external_embedings,\n",
    "        external_embedings_len = args.external_embedings_len,\n",
    "        emb_model_checkpoint = args.emb_model_checkpoint,\n",
    "        feat_concat = args.feat_concat)\n",
    "    model = Classifier(model_config)\n",
    "\n",
    "    if args.load_backbone:\n",
    "       model.load_backbone_checkpoint(args.backbone_checkpoint_path_load)\n",
    "\n",
    "    test_dataset, train_dataset, valid_dataset = build_datasets(args.dataset_name, args.dataset_root_dir, input_size=args.input_size)\n",
    "\n",
    "    model = train(model, train_dataset, valid_dataset, args)\n",
    "\n",
    "    if args.save_backbone:\n",
    "        model.save_backbone_checkpoint(args.backbone_checkpoint_path_load)\n",
    "\n",
    "    if args.test_after_train:\n",
    "        test(model, train_dataset, valid_dataset, test_dataset, args)\n",
    "\n",
    "# train_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "# !pip install setuptools\n",
    "# !pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main for train\n",
    "import argparse\n",
    "from classifier import ClfConfig, Classifier\n",
    "from utils import train, test\n",
    "from data.data_utils import build_datasets\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "def test_main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output_dir', default=\"./Classifier\")\n",
    "    parser.add_argument('--evaluation_strategy', default=\"steps\")\n",
    "    parser.add_argument('--logging_steps', default=50)\n",
    "    parser.add_argument('--save_steps', default=50)\n",
    "    parser.add_argument('--eval_steps', default=50)\n",
    "    parser.add_argument('--save_total_limit', default=3)\n",
    "    parser.add_argument('--report_to', default=\"wandb\")\n",
    "    parser.add_argument('--run_name', default=\"clf_test\")\n",
    "    parser.add_argument('--dataloader_num_workers', default=16)\n",
    "    parser.add_argument('--lr_scheduler_type', default=\"linear\")\n",
    "    parser.add_argument('--learning_rate', default=2e-5)\n",
    "    parser.add_argument('--batch_size', default=16)\n",
    "    parser.add_argument('--gradient_accumulation_steps', default=4)\n",
    "    parser.add_argument('--num_train_epochs', default=15)\n",
    "    parser.add_argument('--warmup_ratio', default=0.02)\n",
    "    parser.add_argument('--metric_for_best_model', default=\"kappa\")\n",
    "    # parser.add_argument('--with_emdedings', default=None)\n",
    "    \n",
    "    parser.add_argument('--plots_path', default=\"src\")\n",
    "    parser.add_argument('--save_hgf_model', default=True)\n",
    "    parser.add_argument('--saved_model_dir', default=\"checkpoints\")\n",
    "    parser.add_argument('--backbone_name', default=\"resnet50\")\n",
    "    parser.add_argument('--num_classes', default=5)\n",
    "    parser.add_argument('--input_size', default=224)\n",
    "    parser.add_argument('--pretrained', default=True)\n",
    "    parser.add_argument('--external_embedings', default=False)\n",
    "    parser.add_argument('--external_embedings_len', default=384)\n",
    "    parser.add_argument('--feat_concat', default=True)\n",
    "    parser.add_argument('--load_backbone', default=False)\n",
    "    parser.add_argument('--backbone_checkpoint_path_load', default=\"checkpoints/resnet50_backbone.pt\")\n",
    "    parser.add_argument('--dataset_name', default=\"DDR\")\n",
    "    parser.add_argument('--backbone_checkpoint_path_save', default=\"checkpoints/resnet50_backbone.pt\")\n",
    "    parser.add_argument('--test_after_train', default=True)\n",
    "    parser.add_argument('--clf_checkpoint_path', default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = Classifier.from_pretrained(args.clf_checkpoint_path)\n",
    "\n",
    "    test_dataset, train_dataset, valid_dataset = build_datasets(args.dataset_name, input_size=args.input_size)\n",
    "\n",
    "    test(model, train_dataset, valid_dataset, test_dataset, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
